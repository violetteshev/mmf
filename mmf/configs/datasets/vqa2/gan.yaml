dataset_config:
  gan_vqa2:
    data_dir: ${env.data_dir}/datasets
    depth_first: false
    fast_read: false
    use_images: true
    add_answer: false
    use_features: false
    zoo_requirements:
    - coco.defaults
    - vqa2.defaults
    images:
      train:
      - coco/defaults/images/train2014
      - coco/defaults/images/val2014
      val:
      - coco/defaults/images/val2014
      test:
      - coco/defaults/images/val2014
    annotations:
      train:
      - vqa2/defaults/annotations/imdb_gan_train.npy
      - vqa2/defaults/annotations/imdb_gan_nominival.npy
      val:
      - vqa2/defaults/annotations/imdb_gan_minival.npy
      test:
      - vqa2/defaults/annotations/imdb_gan_minival.npy
    max_features: 100
    max_pred_ans: 2
    use_image_feature_masks: false
    processors:
      answer_processor:
          type: vqa_answer
          params:
            num_answers: 10
            vocab_file: vqa2/defaults/extras/vocabs/answers_vqa.txt
            preprocessor:
              type: simple_word
              params: {}
      text_processor:
        type: vocab
        params:
          max_length: 20
          vocab:
            type: gan
            vocab_file: vqa2/defaults/extras/vocabs/coco_gan_vocab.txt
          preprocessor:
            type: simple_sentence
            params: {}
      image_processor:
          type: torchvision_transforms
          params:
            transforms:
            - type: Resize
              params:
                  size: [304, 304]
            - type: RandomCrop
              params:
                size: [256, 256]
            - RandomHorizontalFlip
            - ToTensor
            - type: Normalize
              params:
                mean: [0.5, 0.5, 0.5]
                std: [0.5, 0.5, 0.5]
    return_features_info: false
    # Return OCR information
    use_ocr: false
    # Return spatial information of OCR tokens if present
    use_ocr_info: false
